{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553b96e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import re\n",
    "import torch\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "from torch import Tensor\n",
    "from collections import defaultdict\n",
    "\n",
    "class Lars(Optimizer):\n",
    "    def __init__(\n",
    "        self, \n",
    "        net,\n",
    "        params, \n",
    "        lr=required, \n",
    "        momentum=0, \n",
    "        eta=1e-3, \n",
    "        dampening=0,\n",
    "        weight_decay=0, \n",
    "        exclude_from_weight_decay = ['batchnorm', 'bias'],\n",
    "        exclude_from_layer_adaptation = None,\n",
    "        ratio_clip_value = 50.,\n",
    "        decay_grad_clip_value = 10.,\n",
    "        nesterov=False, \n",
    "        epsilon=1e-5,\n",
    "    ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if momentum < 0.0:\n",
    "            raise ValueError(\"Invalid momentum value: {}\".format(momentum))\n",
    "        if eta < 0.0:\n",
    "            raise ValueError(\"Invalid eta value: {}\".format(eta))\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(\"Invalid weight_decay value: {}\".format(weight_decay))\n",
    "        if ratio_clip_value is not None and ratio_clip_value < 0.0:\n",
    "            raise ValueError('Invalid ratio_clip_value: {}'.format(ratio_clip_value))\n",
    "        if decay_grad_clip_value is not None and decay_grad_clip_value < 0.0:\n",
    "            raise ValueError(\n",
    "                'Invalid decay_grad_clip_value: {}'.format(decay_grad_clip_value)\n",
    "            )\n",
    "        if nesterov and (momentum <= 0 or dampening != 0):\n",
    "            raise ValueError(\"Nesterov momentum requires a momentum and zero dampening\")\n",
    "        defaults = dict(lr=lr, momentum=momentum, eta=eta, dampening=dampening,\n",
    "                        weight_decay=weight_decay, nesterov=nesterov, epsilon=epsilon)\n",
    "\n",
    "        self.net = net\n",
    "        self.exclude_from_weight_decay = exclude_from_weight_decay\n",
    "        # `exclude_from_layer_adaptation` is set to be the same as\n",
    "        # `exclude_from_weight_decay` if it is None.\n",
    "        # Borrow from official tensorflow LAMB implementation\n",
    "        if exclude_from_layer_adaptation:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_layer_adaptation\n",
    "        else:\n",
    "            self.exclude_from_layer_adaptation = exclude_from_weight_decay\n",
    "        self.ratio_clip_value = ratio_clip_value\n",
    "        self.decay_grad_clip_value = decay_grad_clip_value\n",
    "        super(Lars, self).__init__(params, defaults)\n",
    "        self._check()\n",
    "        self._init_paraName()\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(Lars, self).__setstate__(state)\n",
    "        for group in self.param_groups:\n",
    "            group.setdefault('nesterov', False)\n",
    "    \n",
    "    def _check(self):\n",
    "        r'''\n",
    "        Check if all `params` are in `net`\n",
    "        '''\n",
    "        netDict = defaultdict(dict)\n",
    "        for p in self.net.parameters():\n",
    "            netDict[p]=True\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if netDict.get(p) is None:\n",
    "                    msg = (\n",
    "                        'All `params` must be in `net` '\n",
    "                        'but got unexpected parameter(s). '\n",
    "                        'Please check.' \n",
    "                    )\n",
    "                    raise RuntimeError(msg)\n",
    "        del netDict\n",
    "\n",
    "    def _init_paraName(self):\n",
    "        r'''\n",
    "        Get all parameters' name in `self.net` and then store it in `self.state`\n",
    "        Do it in initialzation.\n",
    "        '''\n",
    "        for name,para in self.net.named_parameters():\n",
    "            module_top2bottom = name.split('.')\n",
    "            cursor = self.net\n",
    "            for i in range(len(module_top2bottom)-1):\n",
    "                cursor = getattr(cursor, module_top2bottom[i])\n",
    "            bottom_m_name = repr(cursor).split('(')[0]\n",
    "            this_para_name = '.'.join([bottom_m_name, module_top2bottom[-1]])\n",
    "            \n",
    "            # Adding name for each parameter\n",
    "            # e.g. Conv2d.weight, BatchNorm2d.bias, etc.\n",
    "            # This is for `exclude_weight_dacay` and `exculde_layer_adaptation`\n",
    "            self.state[para]['para_name'] = this_para_name\n",
    "\n",
    "    def _do_layer_adaptation(self, para):\n",
    "        r\"\"\"\n",
    "        Whether to do layer-wise learning rate adaptation for `para`.\n",
    "        \"\"\"\n",
    "        para_name = self.state[para]['para_name']\n",
    "        if self.exclude_from_layer_adaptation:\n",
    "            for r in self.exclude_from_layer_adaptation:\n",
    "                if re.search(r, para_name, re.I) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "    \n",
    "    def _do_use_weight_decay(self, para):\n",
    "        r\"\"\"Whether to use L2 weight decay for `param`.\"\"\"\n",
    "        para_name = self.state[para]['para_name']\n",
    "        if self.exclude_from_weight_decay:\n",
    "            for r in self.exclude_from_weight_decay:\n",
    "                if re.search(r, para_name, re.I) is not None:\n",
    "                    return False\n",
    "        return True\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def step(self, closure=None):\n",
    "        \"\"\"Performs a single optimization step.\n",
    "        Arguments:\n",
    "            closure (callable, optional): A closure that reevaluates the model\n",
    "                and returns the loss.\n",
    "        \"\"\"\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            weight_decay = group['weight_decay']\n",
    "            momentum = group['momentum']\n",
    "            eta = group['eta']\n",
    "            dampening = group['dampening']\n",
    "            nesterov = group['nesterov']\n",
    "            epsilon = group['epsilon']\n",
    "\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                if self._do_layer_adaptation(p):\n",
    "                    wd_coff = weight_decay if self._do_use_weight_decay(p) else 1.0\n",
    "                    w_norm = torch.norm(p.data)\n",
    "                    g_norm = torch.norm(p.grad.data)\n",
    "                    if w_norm * g_norm > 0:\n",
    "                        trust_ratio = eta * w_norm / (g_norm +\n",
    "                            wd_coff * w_norm + epsilon)\n",
    "                    else:\n",
    "                        trust_ratio = 1\n",
    "                else:\n",
    "                    trust_ratio = 1\n",
    "                if self.ratio_clip_value is not None:\n",
    "                    trust_ratio = min(trust_ratio, self.ratio_clip_value)\n",
    "                \n",
    "                d_p = p.grad.data\n",
    "                if weight_decay != 0 and self._do_use_weight_decay(p):\n",
    "                    d_p.add_(weight_decay, p.data)\n",
    "                if self.decay_grad_clip_value is not None:\n",
    "                    d_p.clamp_(-self.decay_grad_clip_value, self.decay_grad_clip_value)\n",
    "                \n",
    "                if momentum != 0:\n",
    "                    param_state = self.state[p]\n",
    "                    if 'momentum_buffer' not in param_state:\n",
    "                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n",
    "                    else:\n",
    "                        buf = param_state['momentum_buffer']\n",
    "                    buf.mul_(momentum).add_(1 - dampening, d_p)\n",
    "                    if nesterov:\n",
    "                        d_p = d_p.add(momentum, buf)\n",
    "                    else:\n",
    "                        d_p = buf\n",
    "\n",
    "                p.data.add_(-trust_ratio * group['lr'], d_p)\n",
    "\n",
    "        return loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    '''\n",
    "    A toy model.\n",
    "    '''\n",
    "    import torchvision\n",
    "    resnet = torchvision.models.resnet18(pretrained=False)\n",
    "    optim = Lars(\n",
    "            resnet, \n",
    "            resnet.parameters(), \n",
    "            lr=0.01, \n",
    "            momentum=0.99, \n",
    "            eta=1e-3, \n",
    "            dampening=0,\n",
    "            weight_decay=0.001, \n",
    "            exclude_from_weight_decay = ['bias','batchnorm'],\n",
    "            exclude_from_layer_adaptation = None,\n",
    "            ratio_clip_value = 50.,\n",
    "            decay_grad_clip_value = 10.,\n",
    "        )\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    resnet.zero_grad()\n",
    "    inp = torch.randn(1,3,224,224)\n",
    "    outp = resnet(inp)\n",
    "    target = torch.ones(1,).long()\n",
    "    loss = criterion(outp, target)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # check parameters if they use `weight_decay` or `layerwise adaptation`\n",
    "    state = optim.state\n",
    "    for group in optim.param_groups:\n",
    "        for p in group['params']:\n",
    "            print('[{}] : weight_decay ({}) | layerwise adaptation ({}).'.format(\n",
    "                state[p]['para_name'], optim._do_use_weight_decay(p), optim._do_use_weight_decay(p)\n",
    "            ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
