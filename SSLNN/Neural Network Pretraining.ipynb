{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ecc0c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\deepmimo\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch,gc\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import tensor\n",
    "from math import e\n",
    "\n",
    "Nr=4;Nt=4;K=4\n",
    "channel = torch.from_numpy(np.load('noisechannel.npy',allow_pickle=True))\n",
    "imagechannel = channel.view(18100,Nr**2,Nt**2)\n",
    "#input normalization\n",
    "#Rotate\n",
    "r90channel = torch.zeros(size = (18100,Nr**2,Nt**2),dtype = torch.cfloat)\n",
    "r180channel = torch.zeros(size = (18100,Nr**2,Nt**2),dtype = torch.cfloat)\n",
    "r270channel = torch.zeros(size = (18100,Nr**2,Nt**2),dtype = torch.cfloat)\n",
    "for counter in range(0,18099):\n",
    "    rochannel = imagechannel[counter,:,:]\n",
    "    r90 = np.rot90(rochannel, 1).copy()\n",
    "    r180 = np.rot90(rochannel, 2).copy()\n",
    "    r270 = np.rot90(rochannel, 3).copy()\n",
    "    r90channel[counter,:,:] = torch.from_numpy(r90)\n",
    "    r180channel[counter,:,:] = torch.from_numpy(r180)\n",
    "    r270channel[counter,:,:] = torch.from_numpy(r270)\n",
    "\n",
    "# flip\n",
    "vflip = torch.zeros(size = (18100,Nr**2,Nt**2),dtype = torch.cfloat)\n",
    "hflip = torch.zeros(size = (18100,Nr**2,Nt**2),dtype = torch.cfloat)\n",
    "for counter in range(0,18099):\n",
    "    flipchannel = imagechannel[counter,:,:]\n",
    "    vflip[counter,:,:] = torch.flip(flipchannel,[0])\n",
    "    hflip[counter,:,:] = torch.flip(flipchannel,[1])    \n",
    " \n",
    "    \n",
    "#Gaussian blur\n",
    "noiseamp = 10**-7\n",
    "gnoise = noiseamp*(torch.randn(18100,Nr**2,Nt**2)+torch.randn(18100,Nr**2,Nt**2)*1j)\n",
    "gchannel = imagechannel + gnoise\n",
    "\n",
    "# tranpose and conjugate\n",
    "Tchannel = torch.randn(18100,Nr**2,Nt**2)+torch.randn(18100,Nr**2,Nt**2)*1j\n",
    "Cchannel = torch.randn(18100,Nr**2,Nt**2)+torch.randn(18100,Nr**2,Nt**2)*1j\n",
    "Hchannel = torch.randn(18100,Nr**2,Nt**2)+torch.randn(18100,Nr**2,Nt**2)*1j\n",
    "for counter in range(0,18099):\n",
    "    Tchannel[counter,:,:] = imagechannel[counter,:,:].T\n",
    "    Cchannel[counter,:,:] = imagechannel[counter,:,:].conj()\n",
    "    Hchannel[counter,:,:] = (imagechannel[counter,:,:].T).conj()\n",
    "    \n",
    "#Cut out\n",
    "Ncutout = 2\n",
    "cochannel = channel/abs(channel)\n",
    "for counter in range(0,18099):\n",
    "    cutoutindex = random.sample(range(0,15),Ncutout)\n",
    "    cochannel[cutoutindex[0],counter]=0\n",
    "    cochannel[cutoutindex[1],counter]=0\n",
    "cutoutchannel = cochannel.view(18100,Nr**2,Nt**2)\n",
    "\n",
    "# Normalization\n",
    "data1 = Cchannel/abs(Cchannel)\n",
    "data2 = Hchannel /abs(Hchannel)\n",
    "\n",
    "#user grouping\n",
    "datax = torch.zeros(size=(int(18100/K),2*K,Nr**2,Nt**2),dtype = torch.cfloat)\n",
    "datay = torch.zeros(size=(int(18100/K),2*K,Nr**2,Nt**2),dtype = torch.cfloat)\n",
    "uindex = (np.random.permutation(len(imagechannel))).reshape(-1,K)\n",
    "for i in range(len(uindex)):\n",
    "    for j in range(K):\n",
    "        datax[i,j,:,:] = torch.real(data1[uindex[i,j],:,:])\n",
    "        datax[i,j+K,:,:] = torch.imag(data1[uindex[i,j],:,:])\n",
    "        datay[i,j,:,:] = torch.real(data2[uindex[i,j],:,:])\n",
    "        datay[i,j+K,:,:] = torch.imag(data1[uindex[i,j],:,:])\n",
    "        \n",
    "minibatch = 128\n",
    "trainset = Data.TensorDataset(datax,datay)\n",
    "train_loader = Data.DataLoader(dataset = trainset, batch_size = minibatch, shuffle=True, num_workers = 2, drop_last = True)\n",
    "\n",
    "del channel,imagechannel,gchannel,Tchannel,Hchannel,data1,data2\n",
    "\n",
    "\n",
    "#N_training = int(0.7*len(imagechannel))\n",
    "#N_testing = len(imagechannel)- N_training\n",
    "#training_index = random.sample(range(0,len(imagechannel)),N_training)\n",
    "#testing_index = random.sample(range(0,len(imagechannel)),N_testing)\n",
    "#train_loader = torch.utils.data.DataLoader(traindata, batch_size=64, shuffle=True)\n",
    "#test_loader = torch.utils.data.DataLoader(testdata, batch_size=64, shuffle=True)9.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3539fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### define the device to use\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "learning_rate = 0.001\n",
    "class network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(network, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(8, 8, kernel_size=2, stride=1)\n",
    "        self.bn1 = nn.BatchNorm2d(8)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=2, stride=1)\n",
    "        self.bn2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=2, stride=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, kernel_size=2, stride=1)\n",
    "        self.conv5 = nn.Conv2d(32, 64, kernel_size=2, stride=1)\n",
    "        self.pool = nn.MaxPool2d(4,4)\n",
    "        self.fc1 = nn.Linear(256,512)\n",
    "        self.relu1 = nn.ReLU(inplace=False)\n",
    "        self.fc2 = nn.Linear(512,512)\n",
    "        self.relu2 = nn.ReLU(inplace=False)\n",
    "        self.fc3 = nn.Linear(512,256)\n",
    "        self.relu3 = nn.ReLU(inplace=False)\n",
    "        self.fc4 = nn.Linear(256,64)\n",
    "\n",
    "        #self.softmax = nn.Softmax(dim=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = (self.conv3(x))\n",
    "        x = (self.conv4(x))\n",
    "        x = (self.conv5(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x= self.fc4(x)\n",
    "        #x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "model = network().to(device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate,weight_decay=1e-2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01,amsgrad=False)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=False, threshold=0.001, threshold_mode='rel', cooldown=0, min_lr=0.000001, eps=1e-08)\n",
    "#ExpLR = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.1, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9aa8f83",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Step:  0 | Loss:  5.537646293640137 |Time 73.66018748283386\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  1 | Loss:  5.523474216461182 |Time 73.42072224617004\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  2 | Loss:  5.41547966003418 |Time 73.25115275382996\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  3 | Loss:  5.357720375061035 |Time 73.60224175453186\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  4 | Loss:  5.0622711181640625 |Time 73.94033455848694\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  5 | Loss:  5.221648216247559 |Time 74.82106399536133\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  6 | Loss:  5.08548641204834 |Time 74.04702401161194\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  7 | Loss:  4.7340240478515625 |Time 74.26147890090942\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  8 | Loss:  4.621762275695801 |Time 74.46994996070862\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  9 | Loss:  4.511107444763184 |Time 74.47091889381409\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  10 | Loss:  4.467177867889404 |Time 74.32730317115784\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  11 | Loss:  4.4289655685424805 |Time 74.2116129398346\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  12 | Loss:  4.338414192199707 |Time 74.51482915878296\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  13 | Loss:  4.288288116455078 |Time 74.11088132858276\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  14 | Loss:  4.275821685791016 |Time 74.43501543998718\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  15 | Loss:  4.217436790466309 |Time 73.73890495300293\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  16 | Loss:  4.204022407531738 |Time 73.93834328651428\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  17 | Loss:  4.168493270874023 |Time 73.81168150901794\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  18 | Loss:  4.128424644470215 |Time 73.97324919700623\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  19 | Loss:  4.124050140380859 |Time 73.62817168235779\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  20 | Loss:  4.104356288909912 |Time 73.86456537246704\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  21 | Loss:  4.114031791687012 |Time 74.30735778808594\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  22 | Loss:  4.088418960571289 |Time 73.87750101089478\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  23 | Loss:  4.184021949768066 |Time 74.09575247764587\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  24 | Loss:  4.036045551300049 |Time 73.43234491348267\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  25 | Loss:  4.050008296966553 |Time 74.0132429599762\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  26 | Loss:  4.000001430511475 |Time 74.11686635017395\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  27 | Loss:  3.987989902496338 |Time 74.15379548072815\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  28 | Loss:  3.9682834148406982 |Time 74.2455222606659\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  29 | Loss:  3.9629366397857666 |Time 73.61736607551575\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  30 | Loss:  3.990863800048828 |Time 73.08957815170288\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  31 | Loss:  3.9612555503845215 |Time 73.81569910049438\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  32 | Loss:  3.966348648071289 |Time 73.40274715423584\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  33 | Loss:  3.9564530849456787 |Time 73.24123191833496\n",
      "Learning rate: 0.001\n",
      "Epoch:  0 | Step:  34 | Loss:  3.929440975189209 |Time 73.85956239700317\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  0 | Loss:  3.9214963912963867 |Time 73.67006039619446\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  1 | Loss:  3.8981525897979736 |Time 73.62121868133545\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  2 | Loss:  3.9206066131591797 |Time 73.71232485771179\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  3 | Loss:  3.90291690826416 |Time 74.08496880531311\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  4 | Loss:  3.9150094985961914 |Time 73.54495239257812\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  5 | Loss:  3.895050287246704 |Time 74.27367329597473\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  6 | Loss:  3.864532709121704 |Time 74.23155546188354\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  7 | Loss:  3.8842544555664062 |Time 73.5234522819519\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  8 | Loss:  3.898590564727783 |Time 74.51582765579224\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  9 | Loss:  3.862189292907715 |Time 73.95925688743591\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  10 | Loss:  3.875871181488037 |Time 74.25150489807129\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  11 | Loss:  3.8762378692626953 |Time 97.42067122459412\n",
      "Learning rate: 0.001\n",
      "Epoch:  1 | Step:  12 | Loss:  3.88692045211792 |Time 101.59962511062622\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  13 | Loss:  3.874312400817871 |Time 120.62879371643066\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  14 | Loss:  3.837728500366211 |Time 110.9298985004425\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  15 | Loss:  3.8345837593078613 |Time 107.52057814598083\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  16 | Loss:  3.8266067504882812 |Time 104.2739028930664\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  17 | Loss:  3.8453712463378906 |Time 104.65803861618042\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  18 | Loss:  3.8102400302886963 |Time 103.865567445755\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  19 | Loss:  3.832468032836914 |Time 104.80361461639404\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  20 | Loss:  3.8145859241485596 |Time 107.43530440330505\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  21 | Loss:  3.8163211345672607 |Time 107.35197496414185\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  22 | Loss:  3.811248779296875 |Time 131.95844984054565\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  23 | Loss:  3.823169708251953 |Time 133.32407903671265\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  24 | Loss:  3.794952154159546 |Time 132.6999933719635\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  25 | Loss:  3.8166840076446533 |Time 131.4216046333313\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  26 | Loss:  3.8121867179870605 |Time 134.35057830810547\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  27 | Loss:  3.806495428085327 |Time 133.14480686187744\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  28 | Loss:  3.783215045928955 |Time 134.5727026462555\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  29 | Loss:  3.805105686187744 |Time 134.7827501296997\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  30 | Loss:  3.7878923416137695 |Time 128.80050706863403\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  31 | Loss:  3.7852559089660645 |Time 139.84932827949524\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  32 | Loss:  3.8001010417938232 |Time 117.73179459571838\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  33 | Loss:  3.784250259399414 |Time 113.29954981803894\n",
      "Learning rate: 0.0005\n",
      "Epoch:  1 | Step:  34 | Loss:  3.7951817512512207 |Time 135.30634188652039\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  0 | Loss:  3.7935476303100586 |Time 134.410315990448\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  1 | Loss:  3.7849555015563965 |Time 133.4329354763031\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  2 | Loss:  3.770385980606079 |Time 131.90423107147217\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  3 | Loss:  3.778041362762451 |Time 135.1558539867401\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  4 | Loss:  3.7704625129699707 |Time 133.40166854858398\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  5 | Loss:  3.7647809982299805 |Time 134.30649423599243\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  6 | Loss:  3.7570981979370117 |Time 120.61694669723511\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  7 | Loss:  3.764270067214966 |Time 134.36946988105774\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  8 | Loss:  3.7783491611480713 |Time 136.7322235107422\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  9 | Loss:  3.748300552368164 |Time 135.1152482032776\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  10 | Loss:  3.7665984630584717 |Time 138.13091158866882\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  11 | Loss:  3.7627315521240234 |Time 135.12148666381836\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  12 | Loss:  3.752627372741699 |Time 135.6324644088745\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  13 | Loss:  3.795074939727783 |Time 135.30612444877625\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  14 | Loss:  3.7608890533447266 |Time 119.29223585128784\n",
      "Learning rate: 0.00025\n",
      "Epoch:  2 | Step:  15 | Loss:  3.7558107376098633 |Time 138.56280612945557\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  16 | Loss:  3.75372314453125 |Time 136.90823316574097\n",
      "Learning rate: 0.000125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 | Step:  17 | Loss:  3.7495813369750977 |Time 134.9952142238617\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  18 | Loss:  3.759194850921631 |Time 134.94215607643127\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  19 | Loss:  3.7342236042022705 |Time 133.2136447429657\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  20 | Loss:  3.735128879547119 |Time 135.95192503929138\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  21 | Loss:  3.7397053241729736 |Time 132.85398817062378\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  22 | Loss:  3.750375270843506 |Time 124.83112621307373\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  23 | Loss:  3.758948802947998 |Time 128.7159125804901\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  24 | Loss:  3.7396416664123535 |Time 136.21109700202942\n",
      "Learning rate: 0.000125\n",
      "Epoch:  2 | Step:  25 | Loss:  3.7493011951446533 |Time 134.47489738464355\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  26 | Loss:  3.738426446914673 |Time 136.95299243927002\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  27 | Loss:  3.727853775024414 |Time 135.07366466522217\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  28 | Loss:  3.73551607131958 |Time 140.63115525245667\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  29 | Loss:  3.7346444129943848 |Time 138.71623015403748\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  30 | Loss:  3.740441083908081 |Time 136.498437166214\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  31 | Loss:  3.746225357055664 |Time 136.0193350315094\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  32 | Loss:  3.7417221069335938 |Time 135.69501543045044\n",
      "Learning rate: 6.25e-05\n",
      "Epoch:  2 | Step:  33 | Loss:  3.741131544113159 |Time 135.8357470035553\n",
      "Learning rate: 3.125e-05\n",
      "Epoch:  2 | Step:  34 | Loss:  3.7351009845733643 |Time 136.0774736404419\n",
      "Learning rate: 3.125e-05\n",
      "Epoch:  3 | Step:  0 | Loss:  3.734286308288574 |Time 135.38891792297363\n",
      "Learning rate: 3.125e-05\n",
      "Epoch:  3 | Step:  1 | Loss:  3.7377235889434814 |Time 133.1114957332611\n",
      "Learning rate: 3.125e-05\n",
      "Epoch:  3 | Step:  2 | Loss:  3.750122547149658 |Time 136.6448028087616\n",
      "Learning rate: 3.125e-05\n",
      "Epoch:  3 | Step:  3 | Loss:  3.729989528656006 |Time 134.9144904613495\n",
      "Learning rate: 3.125e-05\n",
      "Epoch:  3 | Step:  4 | Loss:  3.7333786487579346 |Time 124.44630455970764\n",
      "Learning rate: 1.5625e-05\n",
      "Epoch:  3 | Step:  5 | Loss:  3.738276958465576 |Time 122.37197279930115\n",
      "Learning rate: 1.5625e-05\n",
      "Epoch:  3 | Step:  6 | Loss:  3.7348175048828125 |Time 118.69233131408691\n",
      "Learning rate: 1.5625e-05\n",
      "Epoch:  3 | Step:  7 | Loss:  3.724902629852295 |Time 107.59114098548889\n",
      "Learning rate: 1.5625e-05\n",
      "Epoch:  3 | Step:  8 | Loss:  3.7369139194488525 |Time 106.45860004425049\n",
      "Learning rate: 1.5625e-05\n",
      "Epoch:  3 | Step:  9 | Loss:  3.734867811203003 |Time 106.2271580696106\n",
      "Learning rate: 1.5625e-05\n",
      "Epoch:  3 | Step:  10 | Loss:  3.73165225982666 |Time 106.66231656074524\n",
      "Learning rate: 7.8125e-06\n",
      "Epoch:  3 | Step:  11 | Loss:  3.7248878479003906 |Time 107.16441130638123\n",
      "Learning rate: 7.8125e-06\n",
      "Epoch:  3 | Step:  12 | Loss:  3.727968692779541 |Time 107.10111498832703\n",
      "Learning rate: 7.8125e-06\n",
      "Epoch:  3 | Step:  13 | Loss:  3.7358884811401367 |Time 106.62266874313354\n",
      "Learning rate: 7.8125e-06\n",
      "Epoch:  3 | Step:  14 | Loss:  3.7269203662872314 |Time 106.22908020019531\n",
      "Learning rate: 7.8125e-06\n",
      "Epoch:  3 | Step:  15 | Loss:  3.72837233543396 |Time 107.1649603843689\n",
      "Learning rate: 7.8125e-06\n",
      "Epoch:  3 | Step:  16 | Loss:  3.739856004714966 |Time 106.93342709541321\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  17 | Loss:  3.7277674674987793 |Time 106.69349837303162\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  18 | Loss:  3.731156826019287 |Time 106.04691433906555\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  19 | Loss:  3.735405206680298 |Time 106.45544934272766\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  20 | Loss:  3.7362587451934814 |Time 106.01545977592468\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  21 | Loss:  3.723750114440918 |Time 104.94645190238953\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  22 | Loss:  3.7420101165771484 |Time 90.06828188896179\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  23 | Loss:  3.7262396812438965 |Time 89.08711838722229\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  24 | Loss:  3.7597968578338623 |Time 90.36412358283997\n",
      "Learning rate: 3.90625e-06\n",
      "Epoch:  3 | Step:  25 | Loss:  3.727931261062622 |Time 89.75074577331543\n",
      "Learning rate: 3.90625e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m#requires_grad = True\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(loss)\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\deepmimo\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mE:\\anaconda\\envs\\deepmimo\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 4\n",
    "temp = 0.5\n",
    "for epoch in range(num_epochs):\n",
    "    for step, (batchX,batchY) in enumerate(train_loader):\n",
    "        # set zeros\n",
    "        start = time.time()\n",
    "        output = (torch.zeros(2*minibatch,8).to(device)).detach()\n",
    "        upper = ((torch.zeros(minibatch)).to(device)).detach()\n",
    "        lower = ((torch.zeros(minibatch*2,minibatch*2)).to(device)).detach()\n",
    "        loss = (torch.zeros(1).to(device)).detach()\n",
    "        \n",
    "        # get batch\n",
    "        batch = torch.cat((batchX,batchY),0).to(device)\n",
    "        batch = (batch).to(torch.float32).requires_grad_()\n",
    "        output = model(batch)\n",
    "        for i in range(int(len(batch)/2)):\n",
    "            upper[i] = ((e**(((output[i,:] @ output[i+int(len(batch)/2),:])/(torch.norm(output[i,:])* torch.norm(output[i+int(len(batch)/2),:])))/temp)))\n",
    "        for j in range(len(batch)):\n",
    "            for k in range(len(batch)):\n",
    "                lower[j,k] = (e**(((output[j,:] @ output[k,:])/(torch.norm(output[j,:])* torch.norm(output[k,:])))/temp))\n",
    "        loss = torch.sum(-torch.log((upper.repeat(2))/(torch.sum(lower,dim=1)-(upper.repeat(2)))),dim=0)\n",
    "        \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        #loss.requires_grad_(True)\n",
    "        optimizer.zero_grad()\n",
    "        #requires_grad = True\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "        end = time.time()\n",
    "        \n",
    "        print('Epoch: ', epoch, '| Step: ', step, '| Loss: ',float(loss/len(batch)), '|Time',end-start)\n",
    "        #print(model.state_dict()['conv1.weight'])\n",
    "        print('Learning rate:',optimizer.param_groups[0]['lr'])\n",
    "\n",
    "print('Training Finished')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "78d44931",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18100, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "print(Tchannel.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34c3c1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([[[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0'), tensor([[[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([[[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([[[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan], device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0'), tensor([[nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        ...,\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan],\n",
      "        [nan, nan, nan,  ..., nan, nan, nan]], device='cuda:0'), tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       device='cuda:0')]\n",
      "conv1.weight\n",
      "conv1.bias\n",
      "bn1.weight\n",
      "bn1.bias\n",
      "bn1.running_mean\n",
      "bn1.running_var\n",
      "bn1.num_batches_tracked\n",
      "conv2.weight\n",
      "conv2.bias\n",
      "bn2.weight\n",
      "bn2.bias\n",
      "bn2.running_mean\n",
      "bn2.running_var\n",
      "bn2.num_batches_tracked\n",
      "conv3.weight\n",
      "conv3.bias\n",
      "conv4.weight\n",
      "conv4.bias\n",
      "conv5.weight\n",
      "conv5.bias\n",
      "fc1.weight\n",
      "fc1.bias\n",
      "fc2.weight\n",
      "fc2.bias\n",
      "fc3.weight\n",
      "fc3.bias\n",
      "fc4.weight\n",
      "fc4.bias\n",
      "tensor([[[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]],\n",
      "\n",
      "\n",
      "        [[[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]],\n",
      "\n",
      "         [[nan, nan],\n",
      "          [nan, nan]]]], device='cuda:0')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'realbatch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m   \u001b[38;5;28mprint\u001b[39m(name)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mstate_dict()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv1.weight\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# check weights\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mrealbatch\u001b[49m[\u001b[38;5;241m128\u001b[39m,\u001b[38;5;241m0\u001b[39m,:,:]\u001b[38;5;241m.\u001b[39mT)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'realbatch' is not defined"
     ]
    }
   ],
   "source": [
    "print([x.grad for x in optimizer.param_groups[0]['params']]) #check grad\n",
    "for name in model.state_dict():\n",
    "  print(name)\n",
    "print(model.state_dict()['conv1.weight']) # check weights\n",
    "print(realbatch[128,0,:,:].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
